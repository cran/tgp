%\VignetteIndexEntry{example on linear data}
%\VignetteKeywords{tgp}
%\VignetteDepends{tgp,akima,maptree,combinat}
%\VignettePackage{tgp}


\subsection{1-d Linear data}
\label{sec:ex:1dlinear}

<<echo=false,results=hide>>=
library(tgp)
library(akima)
library(maptree)
options(width=70)
@ 

Consider data sampled from a linear model.
\begin{equation} 
z_i = 1 + 2x_i + \epsilon_, \;\;\;\;\; \mbox{where} \;\;\;
\epsilon_i \stackrel{\mbox{\tiny iid}}{\sim} N(0,0.25^2) 
\label{eq:linear:sim}
\end{equation} 

The following {\tt R} code takes a sample $\{\mb{X}, \mb{Z}\}$ of size
$N=50$ from (\ref{eq:linear:sim}).  It also chooses $N'=99$ evenly spaced
predictive locations $\tilde{\mb{X}} = \mbox{\tt XX}$.
<<>>=
# 1-d linear data input and predictive data
X <- seq(0,1,length=50)  # inputs
XX <- seq(0,1,length=99) # predictive locations
Z <- 1 + 2*X + rnorm(length(X),sd=0.25) # responses
@ 

Using {\tt tgp} on this data with a Bayesian
hierarchical linear model goes as follows:
<<>>=
lin.blm <- blm(X=X, XX=XX, Z=Z)
@ 
\begin{figure}[ht!]
\centering
<<label=blm,fig=TRUE,echo=TRUE,width=7,height=4.5>>=
plot(lin.blm, main='Linear Model,')
abline(1,2,lty=3,col='blue')
@
\vspace{-0.5cm}
\caption{\footnotesize Posterior predictive distribution using {\tt blm} on
  synthetic linear data: mean and 90\% credible interval.  The actual
  generating lines are shown as blue-dotted.}
\label{f:lin:blm}
\end{figure}

The first group of text printed to {\tt stdout} is a summary of the
prior parameterization.  Then, MCMC progress indicators are printed
every 1,000 rounds.  The linear model is indicated by {\tt
  cor=[0]}.The GUI versions of {\tt R}, on {\tt Windows} or {\tt MacOS
  X}, usually buffers {\tt stdout}, rendering this feature essentially
useless.  In terminal versions, e.g. {\tt Unix}, the progress
indicators can give a sense of when the code will finish.  Also note
that a user cannot interact while the {\tt C} code is running.  This
will be changed in future versions.

The generic {\tt plot} method can be used to visualize the fitted
posterior predictive surface in terms of means and credible intervals.
Figure \ref{f:lin:blm} shows how to do it, and what you get.  

If, say, you were unsure about the dubious ``linearness'' of this
data, you might try a GP LLM (using {\tt btgpllm}) and let a more
flexible model speak as to the linearity of the process.
<<>>=
lin.gpllm <- bgpllm(X=X, XX=XX, Z=Z)
@
\begin{figure}[ht!]
\centering
<<label=gplm,fig=TRUE,echo=TRUE,width=7,height=4.5>>=
plot(lin.gpllm, main='GP LLM,')
abline(1,2,lty=4,col='blue')
@
\vspace{-0.5cm}
\caption{\footnotesize Posterior predictive distribution using {\tt
    bgpllm} on synthetic linear data: mean and 90\% credible interval.
  The actual generating lines are shown as blue-dotted.}
\label{f:lin:gpllm}
\end{figure}
Whenever the progress indicators show {\tt corr[0]} the process is
under the LLM in that round, and the GP otherwise.  A plot of the
resulting surface is shown in Figure \ref{f:lin:gpllm} for comparison.
Since the data is linear, the resulting predictive surfaces should
look strikingly similar to one another.  On occasion, the GP LLM may
find some bendy-ness in the surface.  This happens rarely with samples
as large as $N=50$, but is quite a bit more common for $N<20$.
